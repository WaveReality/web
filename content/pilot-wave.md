+++
bibfile = "mechphys.json"
+++

When David Bohm reinvented the then-neglected pilot-wave framework of de Broglie, he naturally applied it within the prevalent framework of the time, namely the [[Schrodinger]] wave equation operating within non-local, high-dimensional [[configuration space]]. He was able to show that you can use the gradient of the Schrodinger wave to guide the motion of discrete particles through 3D space, without ever requiring a final wave collapse event. 

Instead, the final positions of these particles represent the predicted outcome of an experiment, and, critically, you have to perform many different "runs" of the experiment with the particles starting in different locations that would be consistent with the actual experimental uncertainty in starting state, as captured in the initial configuration of the Schrodinger wave function. Furthermore, you can simply include the "measurement apparatus" as one of the elements in your configuration space, to get specific predictions about the position of a needle or other readout device.

On the one hand, this is a startling result from the Copenhagen perspective: all that crazy stuff about reality not existing until you measure it, and the non-physical nature of wave function collapse can just be dispensed with entirely. However, many scientists, including Einstein, rejected Bohm's new pilot-wave theory _specifically because of its use of the high-dimensional configuration space_ ([[@NorsenMarianOriols15]]). Why did they not similarly complain about the exclusive use of this same objectionable device in standard QM frameworks? Or in standard classical [[Hamiltonian]] applications?

The answer is evidently that, unlike these other applications, the pilot-wave framework strongly requires that the wave function is somehow a _real_ thing! It must actually influence the real trajectories of particles as they move through space, and thus it must actually be something real itself. And if it is real, then the requirement that it be this strange high-dimensional, non-local beast is just as (if not more) unsatisfying as all the bizarre aspects of the standard framework.

But wait a second. If this very same objectionable configuration space is being used in the standard framework, and it is somehow determining the probabilities for where things end up in actual real experiments, then _why isn't it just as real for the standard framework as well_?  This seems like a serious double standard if there ever was one.

It is really just a verbal legerdemain that avoids the obvious conclusion that if there is an actual interference effect being observed, there must be an actual _cause_ of that interference effect, and that cause cannot just be wished away with a bunch of vague platitudes about complementarity principles etc.

